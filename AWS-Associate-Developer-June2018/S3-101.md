
# S3-101

    "Simple Storage Service"
    Object Storage only
    Data is spread over multiple facilities and devices.  Data is highly available.
    0 bytes to 5 TB
    Unlimited storage
    Bucket names must be unique globally.  The S3 namespace is universal.
    E.g. https://s3-eu-west-1.amazonaws.com/acoudguru
    A successful upload into S3 results in a HTTP 200.

## Data Consistency Model
    "Read after Write" consistency for PUTS of new objects.  You can read your file immediately after uploading it!
    "Eventual Consistency" for overwrites of PUTS and DELETES.  

## General
    S3 is a key-value store
    Key is the name of the object
    Value is the data.
    Version ID is used for versioning.
    Metadata
    Subresources include "Policies", "Access Control Lists", "Cross Origin Resource Sharing" and "Transfer Acceleration".
    
## SLAs
    S3 is designed for 99.99% availability.
    Amazon guarantee 99.9% availability.
    Amazon guarantee 99.(11 9's)% durability.

## Features
### Tiered Storage
### Lifecycle Management
### Versioning
 
	Versioning can be enabled at bucket level. 
	If enabled, S3 keeps every previous version of a file after an PUT/DELETE operation applied to it. 
	PUTtting the same file creates a new version of the latest and set it as the latest version.
	DELETE operation simply puts a marker (delete marker) as the latest version of the target file. 
	Therefore, it's similar to PUTting a delete marker as the latest version.
	
	Standard, GET operation simply returns the latest version to the client.
	Because a DELETE operation means inserting a marker as the latest version, client see the file as deleted.
	
	In order to DELETE a specific version of a file, that version should be specified explicitly in the request.
	Otherwise, it refers to the latest version.
	
	If versioning is not enabled for the bucket, then the simply is physically overwritten with the PUTted version.
	Same as DELETE actually deletes the file.
	
	One common point to remember with versioning, is previous versions of the files also use storage capacity, so
	will be counted on your storage limit as well. As an exception DELETE MARKER only requires space with respect to
	the length of the key name you used for the actual file.
	
### Encryption	
### ACLs and Policies
    
## Storage Tiers

### S3
    99.99% availability
    99.(11 9's)% durability
    Data storage across multiple devices and facilities.
    Designed for loss of 2 facilities.

### S3-IA
    Designed for less frequently accessed data.
    Supports rapid access
    Lower fee but you are charged a retrieval fee.
    
### S3 One Zone IA
    Same as S3-IA but stored on one AZ.
    99.5% availability
    99.(11 9's)% durability
    Cost is 20% less that S3-IA.
    
### Reduced Redundancy Storage
    99.99% availability
    99.99% durability
    For data that can be easily recreated (e.g. thumbnails).
    
### Glacier
    For data that is archived.
    Recovery time is 3-5 hours.
    Very cheap.
    
### S3 Intelligent Tiering
    For data with unknown or unpredictable access pattern.
    2 tiers - Frequent and Infrequent Access
    AWS moves your data to the most cost-effective tier based on how frequently your data is accessed.
    99.9% availability
    99.(11 9's)% durability
    Small fee charged per month for each 1000 objects.
    
## S3 Charges
    Charged per GB
    Charged per requests
    Charged per Tags, Analytics and Inventory(?)
    Charged per download from S3. There is no charge for IN traffic to S3.
    Charged for "Transfer Acceleration".  TA used CloudFront to speed up upload times.
  
## S3 Security
    Buckets are private by default
    "Bucket Policies" applied at bucket level (written in JSON)
    "Access Control Lists" applied at object level.  ACLs describe which accounts/groups have read/write access to objects.
    Buckets can be configured to log all access to a bucket.  Logs can be written to another bucket.
    
## S3 Lab
    When creating a bucket, you can click a checkbox taht will retain all versions of each object in this bucket. 
    When creating a bucket, you can optionally select to encrypt your bucket.
    When uploading objects to your bucket, you may select ACLs and the storage class for this object.
    
## S3 Encryption
    Encryption in Transit
      SSL or TLS
      
    Encryption at Rest
      Server side encryption
        S3 Managed Keys (SSE-S3)
          Each object is encrypted with its own key.
          Each key is then encrypted with a master key which is regularly rotated.
        
        AWS Key Management Service or SSE-KMS
          Each object is encrypted with its own key.
          You are then given access to an "envelope" key which is used to encrypt each of the above keys.
          You also get an audit trail to log the use of your "envelope" key.
          You can supply your own key to use as the envelope key, or you may use the default key which Amazon provide.

        Server side encryption with Customer provided keys (SSE-C)
          AWS perform encryption/decryption but you provide the keys.
      
      Client side encryption
        You encrypt the files yourself before uploading them to S3.
        
### Enforcing encryption on S3 buckets
    Files are uploaded to S3 via HTTP PUT requests.
    "Expect: 100-continue" is a header in the PUT request.  This allows S3 to reject the request before the body has been submitted.
    To request a file to be encrypted on upload, the request must include "x-amz-server-side-encryption: AES256" or "x-amz-server-side-encryption: ams:kms"
    Your bucket policy may require that each header includes "x-amz-server-side-encryption".
    
## Cross Origin Resource Sharing (CORS)
    Allows bucket1 to access resources in bucket2.
    CORS must be enabled on bucket2.
    CORS is enabled by a <CORSConfiguration> block of XML which is applied to bucket2.
    The xml explicitly allows bucket1 to GET bucket2's resources.
    
## CloudFront
    A Content Delivery Network (CDN) is a network of geographically distributed servers that deliver content to a user based on the location of the user.
    
    A CDN is a service that speeds up upload and download times depending on your users' geographic location.  
    
    S3 Transfer Acceleration makes use of CloudFront to achieve faster uploads into S3.
    
    CloudFront employs a collection of geographically distributed servers (Edge Locations) to act as a Cache for Users' objects in S3.
    
    The first time an object is request to S3, Amazon will store that object in a suitable Edge Location.  The next time that object is requested from a user in the same geographic location, it is downloaded from the Edge Location.
    
    Objects remain in an Edge Location for a period known as the Time To live (TTL).
    An Edge Location's cache may be manually cleared (called invalidation). This will incur an extra charge from Amazon.
    
    An "Origin" is the origin of all files that the CDN will distribute.  E.g. S3 Bucket, EC2 Instance, ELB or Route 53.
    A "Distrbution" is the name given to the CDN which contains a collection of Edge Locations.
    
    There are two CloudFront Distribution Types:
       * Web Distribution - Used for web sites.
       * RTMP - Used for Media Streaming.
       
# S3 Performance
    In July 2018, S3 performance was upgraded.
        * 3,500 PUT requests per second
        * 5,500 GET requests per second
        
    No need to randomise your objects keyname in order to prevent hot-spotting of an S3 server.  
    
# From the FAQ

    https://aws.amazon.com/s3/faqs/

    Min file size is 0 bytes
    Max file size is 5 TB
    Max upload size is 5 GB
    If you want to upload a file greater than 5GB then you must use the [multi-part upload API](https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html).
    You can use a multipart upload for objects from 5 MB to 5 TB in size.
    
    The S3 Standard storage class is designed for 99.99% availability
    The S3 Standard-IA storage class is designed for 99.9% availability
    The S3 One Zone-IA storage class is designed for 99.5% availability. 
    
    You specify an AWS Region when you create your Amazon S3 bucket.
    Objects stored in the S3 One Zone-IA storage class are stored redundantly within a single Availability Zone in the AWS Region you select.
    The Amazon S3 One Zone-IA storage class replicates data within a single AZ.
    
    Customers may use four mechanisms for controlling access to Amazon S3 resources: Identity and Access Management (IAM) policies, bucket policies, Access Control Lists (ACLs), and Query String Authentication.
    With Query String Authentication, customers can create a URL to an Amazon S3 object which is only valid for a limited time. 
    
    An Amazon VPC Endpoint for Amazon S3 is a logical entity within a VPC that allows connectivity only to S3.
    
    Amazon Macie is an AI-powered security service that helps you prevent data loss by automatically discovering, classifying, and protecting sensitive data stored in Amazon S3.
    
    Amazon S3 Standard, S3 Standard–IA, S3 One Zone-IA, and S3 Glacier are all designed to provide 99.999999999% durability of objects over a given year.
    For S3 data, best practice includes secure access permissions, Cross-Region Replication, versioning, and a functioning, regularly tested backup.
    
    Amazon S3 Intelligent-Tiering (S3 Intelligent-Tiering) is an S3 storage class for data with unknown access patterns or changing access patterns that are difficult to learn.
    S3 Intelligent-Tiering is designed for the same 99.999999999% durability as S3 Standard.
    S3 Intelligent-Tiering is designed for 99.9% availability.
    There are two ways to get data into S3 Intelligent-Tiering.  You can directly PUT into S3 Intelligent-Tiering by specifying INTELLIGENT_TIERING in the x-amz-storage-class header.  You can set lifecycle policies to transition objects from S3 Standard or S3 Standard-IA to S3 INTELLIGENT_TIERING.
	
    The S3 Standard-IA storage class is set at the object level and can exist in the same bucket as the S3 Standard or S3 One Zone-IA storage classes.
    S3 Standard-IA is designed for the same 99.999999999% durability as the S3 Standard and S3 Glacier storage classes. 
    S3 Standard-IA is designed for 99.9% availability.
    There are two ways to get data into S3 Standard-IA. You can directly PUT into S3 Standard-IA by specifying STANDARD_IA in the x-amz-storage-class header. You can also set Lifecycle policies to transition objects from the S3 Standard to the S3 Standard-IA storage class.
	
    S3 One Zone-IA offers a 99% available SLA and is also designed for eleven 9’s of durability within the Availability Zone. 
    Customers can use S3 One Zone-IA for infrequently-accessed storage, like backup copies, disaster recovery copies, or other easily re-creatable data.
    Amazon S3 One Zone-IA storage is 20% cheaper than Amazon S3 Standard-IA for storage by month.
    As with S3 Standard-Infrequent Access, if you delete a S3 One Zone-IA object within 30 days of creating it, you will incur an early delete charge.
    Like S3 Standard-IA, S3 One Zone-IA storage class has a minimum object size of 128KB.
	
    To retrieve Amazon S3 data stored in the S3 Glacier storage class, initiate a retrieval request using the Amazon S3 APIs or the Amazon S3 Management Console. The retrieval request creates a temporary copy of your data in the S3 RRS or S3 Standard-IA storage class while leaving the archived data intact in S3 Glacier. 
    For all but the largest objects (250MB+), data accessed using Expedited retrievals are typically made available within 1-5 minutes. Objects retrieved using Standard retrievals typically complete between 3-5 hours. Bulk retrievals typically complete within 5-12 hours. 
    Objects that are archived to Amazon S3 Glacier have a minimum of 90 days of storage, and objects deleted before 90 days incur a pro-rated charge equal to the storage charge for the remaining days.
	
    S3 Glacier Deep Archive is a new Amazon S3 storage class that provides secure and durable object storage for long-term retention of data that is accessed once or twice in a year.
    S3 Glacier Deep Archive is an ideal storage class to provide offline protection of your company’s most important data assets.
    S3 Glacier Deep Archive is up to 75% less expensive than S3 Glacier and provides retrieval within 12 hours using the Standard retrieval speed.
    S3 Glacier Deep Archive is designed for the same 99.999999999% durability as the S3 Standard and S3 Glacier storage classes. S3 Glacier Deep Archive is designed for 99.9% availability.
    The easiest way to store data in S3 Glacier Deep Archive is to use the S3 API to upload data directly. Just specify “S3 Glacier Deep Archive” as the storage class. You can accomplish this using the AWS Management Console, S3 REST API, AWS SDKs, or AWS Command Line Interface.  You can also begin using S3 Glacier Deep Archive by creating policies to migrate data using S3 Lifecycle.
    Tape Gateway, a cloud-based virtual tape library feature of AWS Storage Gateway, now integrates with S3 Glacier Deep Archive, enabling you to store your virtual tape-based, long-term backups and archives in S3 Glacier Deep Archive.
    To retrieve data stored in S3 Glacier Deep Archive, initiate a “Restore” request using the Amazon S3 APIs or the Amazon S3 Management Console. The Restore creates a temporary copy of your data in the S3 One Zone-IA storage class while leaving the archived data intact in S3 Glacier Deep Archive.
    Objects that are archived to S3 Glacier Deep Archive have a minimum of 180 days of storage, and objects deleted before 180 days incur a pro-rated charge equal to the storage charge for the remaining days.
	
    With S3 storage management features, you can use a single Amazon S3 bucket to store a mixture of S3 Glacier Deep Archive, S3 Standard, S3 Standard-IA, S3 One Zone-IA, and S3 Glacier data. 
	
    Amazon S3 allows customers to run sophisticated queries against data stored without the need to move data into a separate analytics platform.
    S3 Select is an Amazon S3 feature that makes it easy to retrieve specific data from the contents of an object using simple SQL expressions without having to retrieve the entire object.
    With S3 Select, you can also perform operational investigations on log files in Amazon S3 without the need to operate or manage a compute cluster.
	
    Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL queries. Athena is serverless.
    To get started, just log into the Athena Management Console, define your schema, and start querying.
	
    Amazon Redshift Spectrum is a feature of Amazon Redshift that enables you to run queries against exabytes of unstructured data in Amazon S3 with no loading or ETL required.
	
    Amazon S3 event notifications can be sent in response to actions in Amazon S3 like PUTs, POSTs, COPYs, or DELETEs. Notification messages can be sent through either Amazon SNS, Amazon SQS, or directly to AWS Lambda.
    Amazon S3 event notifications enable you to run workflows, send alerts, or perform other actions in response to changes in your objects stored in S3.
    There are no additional charges for using Amazon S3 for event notifications. You pay only for use of Amazon SNS or Amazon SQS to deliver event notifications, or for the cost of running an AWS Lambda function.
    
    Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and your Amazon S3 bucket.  As data arrives at an AWS Edge Location, data is routed to your Amazon S3 bucket over an optimized network path.
    To get started with S3 Transfer Acceleration enable S3 Transfer Acceleration on an S3 bucket using the Amazon S3 console, the Amazon S3 API, or the AWS CLI. After S3 Transfer Acceleration is enabled, you can point your Amazon S3 PUT and GET requests to the s3-accelerate endpoint domain name.
    
    You can add tags to new objects when you upload them or you can add them to existing objects. Up to ten tags can be added to each S3 object.
    Note that all changes to tags outside of the AWS Management Console are made to the full tag set. If you have five tags attached to a particular object and want to add a sixth, you need to include the original five tags in that request.
    
    The S3 Inventory report provides a scheduled alternative to Amazon S3’s synchronous List API. 
    You can use the AWS Management Console or the PUT Bucket Inventory API to configure a daily or weekly inventory report for all the objects within your S3 bucket or a subset of the objects under a shared prefix.

    S3 Batch Operations is a feature that you can use to automate the execution, management, and auditing of a specific S3 request or Lambda function across many objects stored in Amazon S3.
    You should use S3 Batch Operations if you want to automate the execution of a single operation (like copying an object, or executing an AWS Lambda function) across many objects.
    
    Amazon S3 Object Lock is a new Amazon S3 feature that blocks object version deletion during a customer-defined retention period so that you can enforce retention policies as an added layer of data protection or for regulatory compliance.
    
    CloudWatch Request Metrics will be available in CloudWatch within 15 minutes after they are enabled.
    
    Cross-Region Replication is an Amazon S3 feature that automatically replicates data between AWS Regions
    CRR is configured at the S3 bucket level.
    Versioning must be enabled for both the source and destination buckets to enable CRR.
    You can configure separate S3 Lifecycle rules on the source and destination buckets. For example, you can configure a lifecycle rule to migrate data from the S3 Standard storage class to the S3 Standard-IA or S3 One Zone-IA storage class or archive data to S3 Glacier on the destination bucket.
    You can set up CRR across AWS accounts to store your replicated data in a different account in the target region.
